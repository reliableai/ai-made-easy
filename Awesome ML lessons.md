


# Basics of statistics and probability for ML

MLVU Lesson 8:

- Awesome [lecture on the Normal distribution, with historical context and a lot of examples] (https://www.youtube.com/watch?v=VZfAJzXu1hM)
Love the part where we play being Gauss.

# Entropy and information theory

MLVU Lesson xxx:

- [Entropy and information theory] (xxx)

# Expectation Maximization




# Attention

These are the 3 best lessons on attention.
From Stanford CS224n, I recommend the 2021 class - the most recent generally available.

1. [Final part of lecture 7](https://youtu.be/wzfWHP6SXxY?list=PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ&t=4382) and the [inital part of lecture 8](https://youtu.be/gKD7jPAdbpE?list=PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ&t=113)
2. [Lecture 9](https://youtu.be/6D4EWKJgNn0?list=PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ&t=1)


As an aside, do listen to the entire lecture 8 of 224N. It includes some great practical NLP tips.

You know my passion for Peter Bloem's lectures ([12.1](https://www.youtube.com/watch?v=KmAISyVvE1Y), [12.2](https://www.youtube.com/watch?v=oUhGZMCTHtI) and [12.3](https://www.youtube.com/watch?v=MN__lSncZBs)), and the ones on attention are also fantastic, though they go less deep (since it's not an NLP class) in the historical context and the reasoning for introducing transformers starting from requirements, which is useful to understand not only the intuition behind attention, but also the intuition behind transformers and in general how to approach new kinds of models.



# MCMC



